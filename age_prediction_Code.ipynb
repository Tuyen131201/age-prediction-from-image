{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "421d8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07aa3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= Path('20-50/20-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d7fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_805/2764185558.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ages = pd.Series(filepaths.apply(lambda x: os.path.split(os.path.split(x)[0])[1]), name='Age').astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "filepaths = pd.Series(list(image_path.glob(r'**/*.jpg')), name='Filepath').astype(str)\n",
    "ages = pd.Series(filepaths.apply(lambda x: os.path.split(os.path.split(x)[0])[1]), name='Age').astype(np.int)\n",
    "\n",
    "images = pd.concat([filepaths, ages], axis=1).sample(frac=1.0, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bda3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(images, test_size=0.2, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b7c8c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28855</th>\n",
       "      <td>20-50/20-50/train/47/150836.jpg</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11912</th>\n",
       "      <td>20-50/20-50/train/29/147441.jpg</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>20-50/20-50/train/47/174298.jpg</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36862</th>\n",
       "      <td>20-50/20-50/train/39/149598.jpg</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4077</th>\n",
       "      <td>20-50/20-50/train/34/156051.jpg</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813</th>\n",
       "      <td>20-50/20-50/test/44/42753.jpg</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32511</th>\n",
       "      <td>20-50/20-50/train/26/164073.jpg</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>20-50/20-50/train/40/161263.jpg</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>20-50/20-50/train/22/113418.jpg</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003</th>\n",
       "      <td>20-50/20-50/train/29/162724.jpg</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32352 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Filepath  Age\n",
       "28855  20-50/20-50/train/47/150836.jpg   47\n",
       "11912  20-50/20-50/train/29/147441.jpg   29\n",
       "2666   20-50/20-50/train/47/174298.jpg   47\n",
       "36862  20-50/20-50/train/39/149598.jpg   39\n",
       "4077   20-50/20-50/train/34/156051.jpg   34\n",
       "...                                ...  ...\n",
       "7813     20-50/20-50/test/44/42753.jpg   44\n",
       "32511  20-50/20-50/train/26/164073.jpg   26\n",
       "5192   20-50/20-50/train/40/161263.jpg   40\n",
       "12172  20-50/20-50/train/22/113418.jpg   22\n",
       "33003  20-50/20-50/train/29/162724.jpg   29\n",
       "\n",
       "[32352 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82aea75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ed3606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25882 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "X_train=train_gen.flow_from_dataframe(\n",
    "dataframe=train_df,\n",
    "x_col=\"Filepath\",\n",
    "y_col=\"Age\",\n",
    "target_size=(120,120),\n",
    "color_mode=\"rgb\",\n",
    "class_mode=\"raw\",\n",
    "batch_size=32,\n",
    "shuffle=True,\n",
    "seed=42,\n",
    "subset=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8088 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "X_test=test_gen.flow_from_dataframe(\n",
    "dataframe=test_df,\n",
    "x_col=\"Filepath\",\n",
    "y_col=\"Age\",\n",
    "target_size=(120,120),\n",
    "color_mode=\"rgb\",\n",
    "class_mode=\"raw\",\n",
    "batch_size=32,\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e952dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6470 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "val_images = train_gen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Age',\n",
    "    target_size=(120, 120),\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc608ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 00:50:32.811939: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 00:50:32.818095: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 120, 120, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 118, 118, 16)      448       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 118, 118, 16)     64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 59, 59, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 57, 57, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 57, 57, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 28, 28, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 26, 26, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10816)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1384576   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,416,929\n",
      "Trainable params: 1,416,705\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input=layers.Input(shape=(120,120,3))\n",
    "\n",
    "x=layers.Conv2D(16,3,activation=\"relu\")(input)\n",
    "x=layers.BatchNormalization()(x)\n",
    "x=layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x=layers.Conv2D(32,3,activation=\"relu\")(x)\n",
    "x=layers.BatchNormalization()(x)\n",
    "x=layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x=layers.Conv2D(64,3,activation=\"relu\")(x)\n",
    "x=layers.BatchNormalization()(x)\n",
    "x=layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "x=layers.Flatten()(x)\n",
    "x=layers.Dense(128,activation=\"relu\")(x)\n",
    "x=layers.Dropout(0.3)(x)\n",
    "x=layers.Dense(64,activation=\"relu\")(x)\n",
    "x=layers.Dropout(0.3)(x)\n",
    "\n",
    "output=layers.Dense(1,activation=\"linear\")(x)\n",
    "\n",
    "model=Model(input,output)\n",
    "model.summary()\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c50f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebe8be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "def save_model(model, filename= 'model-dl') :\n",
    "    #serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(filename+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    #serialize weights to HDF5\n",
    "    model.save_weights(filename+'.h5')\n",
    "    print(\"Save model to dick\")\\\n",
    "    #create_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# def create_model(): \n",
    "#     model = Sequential ()\n",
    "#     model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(8,8,1)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(10, activation ='softmax'))\n",
    "#     model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04dab796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "809/809 [==============================] - 282s 347ms/step - loss: 10.0693 - val_loss: 8.9225\n",
      "Epoch 2/50\n",
      "809/809 [==============================] - 303s 374ms/step - loss: 9.0162 - val_loss: 9.1375\n",
      "Epoch 3/50\n",
      "809/809 [==============================] - 320s 396ms/step - loss: 8.7228 - val_loss: 10.9352\n",
      "Epoch 4/50\n",
      "809/809 [==============================] - 268s 331ms/step - loss: 8.5327 - val_loss: 7.5347\n",
      "Epoch 5/50\n",
      "809/809 [==============================] - 291s 360ms/step - loss: 8.2750 - val_loss: 10.1029\n",
      "Epoch 6/50\n",
      "809/809 [==============================] - 277s 343ms/step - loss: 8.1165 - val_loss: 7.4774\n",
      "Epoch 7/50\n",
      "809/809 [==============================] - 276s 341ms/step - loss: 7.9477 - val_loss: 9.2263\n",
      "Epoch 8/50\n",
      "809/809 [==============================] - 453s 560ms/step - loss: 7.8751 - val_loss: 11.1759\n",
      "Epoch 9/50\n",
      "809/809 [==============================] - 298s 367ms/step - loss: 7.6944 - val_loss: 8.0533\n",
      "Epoch 10/50\n",
      "809/809 [==============================] - 300s 371ms/step - loss: 7.5539 - val_loss: 11.6914\n",
      "Epoch 11/50\n",
      "809/809 [==============================] - 271s 334ms/step - loss: 7.4731 - val_loss: 8.2616\n",
      "Epoch 12/50\n",
      "809/809 [==============================] - 277s 343ms/step - loss: 7.2336 - val_loss: 7.2010\n",
      "Epoch 13/50\n",
      "809/809 [==============================] - 278s 343ms/step - loss: 7.0713 - val_loss: 7.4844\n",
      "Epoch 14/50\n",
      "809/809 [==============================] - 280s 346ms/step - loss: 6.9372 - val_loss: 8.9109\n",
      "Epoch 15/50\n",
      "809/809 [==============================] - 310s 383ms/step - loss: 6.7346 - val_loss: 8.4594\n",
      "Epoch 16/50\n",
      "809/809 [==============================] - 272s 337ms/step - loss: 6.5677 - val_loss: 7.6952\n",
      "Epoch 17/50\n",
      "809/809 [==============================] - 282s 349ms/step - loss: 6.4059 - val_loss: 9.1039\n",
      "Epoch 18/50\n",
      "809/809 [==============================] - 262s 324ms/step - loss: 6.2311 - val_loss: 9.3317\n",
      "Epoch 19/50\n",
      "809/809 [==============================] - 265s 327ms/step - loss: 6.0246 - val_loss: 8.8760\n",
      "Epoch 20/50\n",
      "809/809 [==============================] - 279s 345ms/step - loss: 5.9254 - val_loss: 8.8216\n",
      "Epoch 21/50\n",
      "809/809 [==============================] - 273s 337ms/step - loss: 5.8094 - val_loss: 8.0660\n",
      "Epoch 22/50\n",
      "809/809 [==============================] - 272s 336ms/step - loss: 5.6140 - val_loss: 9.2357\n",
      "Epoch 23/50\n",
      "809/809 [==============================] - 273s 337ms/step - loss: 5.4761 - val_loss: 8.0496\n",
      "Epoch 24/50\n",
      "809/809 [==============================] - 266s 329ms/step - loss: 5.3836 - val_loss: 10.8037\n",
      "Epoch 25/50\n",
      "809/809 [==============================] - 274s 339ms/step - loss: 5.2909 - val_loss: 7.7594\n",
      "Epoch 26/50\n",
      "809/809 [==============================] - 283s 350ms/step - loss: 5.1809 - val_loss: 8.8211\n",
      "Epoch 27/50\n",
      "809/809 [==============================] - 263s 324ms/step - loss: 5.0754 - val_loss: 7.7119\n",
      "Epoch 28/50\n",
      "809/809 [==============================] - 290s 358ms/step - loss: 4.9917 - val_loss: 8.3881\n",
      "Epoch 29/50\n",
      "809/809 [==============================] - 285s 352ms/step - loss: 4.8314 - val_loss: 7.4343\n",
      "Epoch 30/50\n",
      "809/809 [==============================] - 271s 335ms/step - loss: 4.7967 - val_loss: 7.6372\n",
      "Epoch 31/50\n",
      "809/809 [==============================] - 271s 334ms/step - loss: 4.7347 - val_loss: 8.4641\n",
      "Epoch 32/50\n",
      "809/809 [==============================] - 278s 344ms/step - loss: 4.7126 - val_loss: 7.9259\n",
      "Epoch 33/50\n",
      "809/809 [==============================] - 269s 333ms/step - loss: 4.6013 - val_loss: 7.7639\n",
      "Epoch 34/50\n",
      "809/809 [==============================] - 267s 330ms/step - loss: 4.5235 - val_loss: 7.9000\n",
      "Epoch 35/50\n",
      "809/809 [==============================] - 282s 349ms/step - loss: 4.5024 - val_loss: 7.9184\n",
      "Epoch 36/50\n",
      "809/809 [==============================] - 261s 323ms/step - loss: 4.4115 - val_loss: 7.9590\n",
      "Epoch 37/50\n",
      "809/809 [==============================] - 269s 332ms/step - loss: 4.3608 - val_loss: 8.3687\n",
      "Epoch 38/50\n",
      "809/809 [==============================] - 284s 351ms/step - loss: 4.2843 - val_loss: 7.2612\n",
      "Epoch 39/50\n",
      "809/809 [==============================] - 356s 441ms/step - loss: 4.2663 - val_loss: 8.3262\n",
      "Epoch 40/50\n",
      "809/809 [==============================] - 283s 349ms/step - loss: 4.2233 - val_loss: 7.7187\n",
      "Epoch 41/50\n",
      "809/809 [==============================] - 276s 341ms/step - loss: 4.1779 - val_loss: 8.3791\n",
      "Epoch 42/50\n",
      "809/809 [==============================] - 270s 333ms/step - loss: 4.1311 - val_loss: 7.6872\n",
      "Epoch 43/50\n",
      "809/809 [==============================] - 274s 339ms/step - loss: 4.0842 - val_loss: 7.5614\n",
      "Epoch 44/50\n",
      "809/809 [==============================] - 285s 352ms/step - loss: 4.0223 - val_loss: 7.4403\n",
      "Epoch 45/50\n",
      "809/809 [==============================] - 272s 336ms/step - loss: 3.9907 - val_loss: 8.4118\n",
      "Epoch 46/50\n",
      "809/809 [==============================] - 270s 334ms/step - loss: 3.9394 - val_loss: 7.2839\n",
      "Epoch 47/50\n",
      "809/809 [==============================] - 283s 349ms/step - loss: 3.9335 - val_loss: 7.7395\n",
      "Epoch 48/50\n",
      "809/809 [==============================] - 272s 336ms/step - loss: 3.8913 - val_loss: 7.5239\n",
      "Epoch 49/50\n",
      "809/809 [==============================] - 256s 316ms/step - loss: 3.8181 - val_loss: 7.9964\n",
      "Epoch 50/50\n",
      "809/809 [==============================] - 277s 343ms/step - loss: 3.7716 - val_loss: 8.1492\n",
      "Save model to dick\n"
     ]
    }
   ],
   "source": [
    "#train_model\n",
    "# model_save= create_model()\n",
    "model.fit(X_train,validation_data=val_images,epochs=50)\n",
    "save_model(model, filename= 'mohinh_dudoantuoi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e3e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "def process_and_predict(file):\n",
    "    im = Image.open(file)\n",
    "    width, height = im.size\n",
    "    if width == height:\n",
    "        im = im.resize((120,120), Image.ANTIALIAS)\n",
    "    else:\n",
    "        if width > height:\n",
    "            left = width/2 - height/2\n",
    "            right = width/2 + height/2\n",
    "            top = 0\n",
    "            bottom = height\n",
    "            im = im.crop((left,top,right,bottom))\n",
    "            im = im.resize((120,120), Image.ANTIALIAS)\n",
    "        else:\n",
    "            left = 0\n",
    "            right = width\n",
    "            top = 0\n",
    "            bottom = width\n",
    "            im = im.crop((left,top,right,bottom))\n",
    "            im = im.resize((120,120), Image.ANTIALIAS)\n",
    "            \n",
    "    ar = np.asarray(im)\n",
    "    ar = ar.astype('float32')\n",
    "    ar /= 255.0\n",
    "    ar = ar.reshape(-1, 120, 120, 3)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    files_to_load = 'mohinh_dudoantuoi'\n",
    "    model_image = load_model(files_to_load)\n",
    "\n",
    "    predictions = model_image.predict(ar)\n",
    "    print('Ket qua du doan - hinh cua ai do:', predictions[0])\n",
    "\n",
    "    \n",
    "#     age = x.predict(ar)\n",
    "        \n",
    "#     print('Age:', int(age))\n",
    "#     return im.resize((300,300), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35cf1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dinh nghia ham/ thu tuc ho tro\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import model_from_json\n",
    "def load_model(filename=\"model-dl\"):\n",
    "    model = model_from_json(open(filename+'.json').read())\n",
    "    model.load_weights(filename+'.h5')\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f815c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_285/3655027125.py:22: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  im = im.resize((120,120), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 93ms/step\n",
      "Ket qua du doan - hinh cua ai do: [33.058113]\n"
     ]
    }
   ],
   "source": [
    "process_and_predict('hinhdudoan/Tet.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716931c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8d43747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nhan_biet_tuoi(images, image_cuaban, classid):\n",
    "    j = 0\n",
    "    flag = 1\n",
    "    fig = plt.figure(figsize= (24,12))\n",
    "    for i in range(len(targets)):\n",
    "        if faces.target[i] == classid:\n",
    "            if j==0:\n",
    "                img_grid = fig.add_subplot(2, 5, j+1)\n",
    "                print('Tuoi cua ban ')\n",
    "                img_grid.imshow(image_cuaban)\n",
    "            else:\n",
    "                img_grid = fig.add_subplot(2,5,j+1)\n",
    "                print('classid,', classid, 'sample_id', i)\n",
    "                img_grid.imshow(images[i])\n",
    "            j=j+1\n",
    "            flag=0\n",
    "        if flag:\n",
    "            print('Khong co anh nao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "905aad27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m anh\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m145207.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m image_cuaban\u001b[38;5;241m=\u001b[39m load_img(anh, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnhan_biet_tuoi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_cuaban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow\n",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m, in \u001b[0;36mnhan_biet_tuoi\u001b[0;34m(images, image_cuaban, classid)\u001b[0m\n\u001b[1;32m      2\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(targets)):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m faces\u001b[38;5;241m.\u001b[39mtarget[i] \u001b[38;5;241m==\u001b[39m classid:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "anh= '145207.jpg'\n",
    "image_cuaban= load_img(anh, target_size=(64,64))\n",
    "nhan_biet_tuoi(images, image_cuaban, classid=5)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bdf6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
